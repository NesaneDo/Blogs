# 基础知识

## 数据操作

### 相关名词

> **张量（tensor）**
>
> n 维数组
>
> 一个轴的张量 ~ 数学上的 向量（vector）【一维数组】
>
> 两个轴的张量 ~ 数学上的 矩阵（matrix）【二维数组】



### 运算符

> **一元标量运算符：$f: \mathbb{R} \rightarrow \mathbb{R}$ **
>
> 只接收一个输入，并产生一个输出。表示该函数可以从任何实数映射到另一个实数

> **二元运算符：$f: \mathbb{R}, \mathbb{R} \rightarrow \mathbb{R}$ **
>
> 接收两个输入，并产生一个输出。

向量点积

矩阵乘法

> **张量连结（concatenate)**
>
> 多个张量可以根据某个轴连起来

### 广播机制

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

### 节省内存

1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。



## 线性代数

### 相关名词

> **标量**：只有一个元素的张量表示

> **向量**：标量值组成的列表，这些标量值被称为向量的元素（element）或分量（component）
>
> 一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制
>
> 向量的长度通常称为向量的维度（dimentsion）

>**矩阵：$\mathbf{A} \in \mathbb{R}^{m \times n}$ **
>
>表示为具有两个轴的张量
>
>当矩阵具有相同数量的行和列时，其形状将变为正方形； 因此，它被称为*方阵*（square matrix）
>
>交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）
>
>方阵的一种特殊类型，*对称矩阵*（symmetric matrix）$\mathbf{A}$ 等于其转置：$\mathbf{A} = \mathbf{A}^\top$ 。

> **点积（Dot Product）**：两个向量（$\mathbf{x},\mathbf{y}\in\mathbb{R}^d$）相同位置的按元素乘积的和
> $$
> \langle\mathbf{x},\mathbf{y}\rangle = \mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i
> $$
> 

> **矩阵-向量积（matrix-vector product）**
> $$
> \begin{split}\mathbf{A}\mathbf{x}
> = \begin{bmatrix}
> \mathbf{a}^\top_{1} \\
> \mathbf{a}^\top_{2} \\
> \vdots \\
> \mathbf{a}^\top_m \\
> \end{bmatrix}\mathbf{x}
> = \begin{bmatrix}
>  \mathbf{a}^\top_{1} \mathbf{x}  \\
>  \mathbf{a}^\top_{2} \mathbf{x} \\
> \vdots\\
>  \mathbf{a}^\top_{m} \mathbf{x}\\
> \end{bmatrix} (\mathbf{A} \in \mathbb{R}^{m \times n}, \mathbf{x} \in \mathbb{R}^n)\end{split}
> $$

> **矩阵-矩阵乘法（matrix-matrix multiplication）**
> $$
> \begin{split}\mathbf{A}=\begin{bmatrix}
>  a_{11} & a_{12} & \cdots & a_{1k} \\
>  a_{21} & a_{22} & \cdots & a_{2k} \\
> \vdots & \vdots & \ddots & \vdots \\
>  a_{n1} & a_{n2} & \cdots & a_{nk} \\
> \end{bmatrix},\quad
> \mathbf{B}=\begin{bmatrix}
>  b_{11} & b_{12} & \cdots & b_{1m} \\
>  b_{21} & b_{22} & \cdots & b_{2m} \\
> \vdots & \vdots & \ddots & \vdots \\
>  b_{k1} & b_{k2} & \cdots & b_{km} \\
> \end{bmatrix}\end{split}
> $$
> 
> $$
> \begin{split}\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
> \mathbf{a}^\top_{1} \\
> \mathbf{a}^\top_{2} \\
> \vdots \\
> \mathbf{a}^\top_n \\
> \end{bmatrix}
> \begin{bmatrix}
>  \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
> \end{bmatrix}
> = \begin{bmatrix}
> \mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
>  \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
>  \vdots & \vdots & \ddots &\vdots\\
> \mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
> \end{bmatrix} (\mathbf{A} \in \mathbb{R}^{n \times k},\mathbf{B} \in \mathbb{R}^{k \times m})\end{split}
> $$
> 

> **范数（norm）**
>
> 向量的*范数*是表示一个向量有多大。 这里考虑的*大小*（size）概念不涉及维度，而是分量的大小
>
> 1. 如果我们按常数因子$\alpha$缩放向量的所有元素， 其范数也会按相同常数因子的*绝对值*缩放
>    $$
>    f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).
>    $$
>    
>2. 满足三角不等式
>    $$
>    f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).
>    $$
>    
> 3. 范数必须是非负的
>   $$
>    f(\mathbf{x}) \geq 0.
>    $$
>    
> 4. 范数最小为0，当且仅当向量全由0组成
>    $$
>   \forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.
>    $$
>    
> 
>    

### 长度、维度和形状

*向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

### 张量算法的基本性质

1. 两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\odot$ ）。

$$
\begin{split}\mathbf{A} \odot \mathbf{B} =
\begin{bmatrix}
    a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
    a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
\end{bmatrix}.\end{split}
$$

2. 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘

   

### 降维

### 非降维求和

### 点积

### 矩阵-向量积

### 矩阵-矩阵乘法

矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与”Hadamard积”混淆

### 范数

范数听起来很像距离的度量。

欧几里得距离是一个$L_2$范数:
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},
$$
在$L_2$范数中常常省略下标2，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$

深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和：
$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.
$$
$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：
$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
$$
类似于向量的$L_2$范数，矩阵$\mathbf{X} \in \mathbb{R}^{m \times n}$的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根
$$
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.
$$

## 微积分

拟合模型的任务的两个关键问题：

1. *优化*（optimization）：用模型拟合观测数据的过程；
2. *泛化*（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型

### 导数和微分

假设我们有一个函数$f: \mathbb{R} \rightarrow \mathbb{R}$，其输入和输出都是标量。 如果$f$的*导数*存在，这个极限被定义为
$$
f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.
$$
计算法则：

常数相乘
$$
\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x),
$$


加法
$$
\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x),
$$


乘法
$$
\frac{d}{dx} [f(x)g(x)] = f(x) \frac{d}{dx} [g(x)] + g(x) \frac{d}{dx} [f(x)],
$$


除法
$$
\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}.
$$

### 偏导数

$y = f(x_1, x_2, \ldots, x_n)$ 是一个具有n个变量的函数, 则 y 关于第 i 个参数 $x_i$ 的偏导数为：
$$
\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.
$$
表示法：
$$
\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.
$$

### 梯度

$$
\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top,
$$

假设 $\mathbf{x}$ 为 n 维向量，在微分多元函数时常使用以下规则：

- 对于所有的 $\mathbf{A} \in \mathbb{R}^{m \times n}$，都有 $\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$
- 对于所有的 $\mathbf{A} \in \mathbb{R}^{n \times m}$，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}$
- 对于所有的 $\mathbf{A} \in \mathbb{R}^{n \times n}$，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$
- $\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$
- 对于矩阵 $\mathbf{X}$，都有 $\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}$



### 链式法则

假设函数$y=f(u)$和$u=g(x)$都是可微的，根据链式法则：
$$
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}
$$
函数具有任意数量的变量的情况。假设可微分函数$y$有变量$u_1, u_2, \ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1,x_2,\ldots,x_n$。（其中 $y$ 是 $x_1,x_2,\ldots,x_n$ 的函数）对于任意$i=1,2,\ldots,n$，链式法则给出：
$$
\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial u_1} \frac{\partial u_1}{\partial x_i} + \frac{\partial y}{\partial u_2} \frac{\partial u_2}{\partial x_i} + \cdots + \frac{\partial y}{\partial u_m} \frac{\partial u_m}{\partial x_i}
$$

- 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。
- 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。
- 梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。
- 链式法则可以用来微分复合函数。

## 自动微分

- 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。

## 概率

